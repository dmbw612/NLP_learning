{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/l0QSKk0XdE7T0aDxgyvw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dmbw612/NLP_learning/blob/master/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Transformer 구조에 대한 논문인 \"Attention is all you need\" 논문에 대해서 살펴보겠습니다.\n",
        "\n",
        "[MODEL에 대하여]\n",
        "\n",
        "RNN은 입력되는 단어 개수만큼 반복되어 encoder가 출력하지만,\n",
        "Transformer에서는 입력 문장이 한번에 입력되어 1회 encoder가 작동하고, decoder에서 <eos>가 나올때까지 반복된다.\n",
        "\n",
        "V개의 단어로 구성된, 그리고 d_model 차원으로 embedding된 한 문장 (V x d_model)을 d_head 차원의 저차원으로 다시 한 번 더 embedding하려고 하는데,\n",
        "이 때 문맥을 고려해서 embedding을 하고 싶다 그래서 QK^T/root(d_head)라는 문맥을 알고 있는 attention energy(?)를 사용한다.\n",
        "결국 원래의 문장은 문맥을 고려한 V x d_head 차원의 attention 문장으로 변경된다.\n",
        "이러한 attention_j 문장들이 h개 만큼 있으므로 concat(attention_1, ..., attention_h)을 수행해서 차원의 attention 문장(V x d_model)을 만들 수 있다.\n",
        "이 attention 문장을 다히 W_o (d_model x d_head)와 행렬곱해주면 V x d_head 차원의 최종 output을 만들 수 있다. (다시 d_head차원으로 변경해야 하는 이유는 decoder에 각 head에 들어가야 하기 때문이다.)\n",
        "이로서 encoder에서의 self-attetion 진행이 끝났다.\n",
        "\n",
        "모델의 한 단어(x_i, (1 x d_model), i=1,...,V) → Q_ij/K_ii'j/V_ii'j (1 x d_head) # i'=1,...,V, j = 1,...,h\n",
        "(d_head = d_model / h, V=max_len, d_model=n_dim)\n",
        "\n",
        "attention energy_i,i' = Q_i (1 x d_head) X K_i'^T (d_head x 1) / root(d_head)\n",
        "attention_i = softmax([ae_i'|i'=1,..,V]) * V_i' (1 x d_head = (1 x d_head) * (1 x d_head))\n",
        "\n"
      ],
      "metadata": {
        "id": "fcaLbISGvone"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVNIcG0xvj1o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "NYRIowmcvmzS"
      }
    }
  ]
}